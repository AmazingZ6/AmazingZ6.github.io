---
title: 差分隐私之组合定理
date: 2022-10-10 17:05:04
tags: 差分隐私
plugins:
  - mathjax
---

# 差分隐私之组合定理

  在了解组合定理前，先要知道差分隐私的三个基本属性，这些是组合定理的基础

## 顺序组合(Sequential Composition)

   差别隐私的第一个主要属性是顺序组合，它限定了在同一输入数据上释放差别隐私机制的多个结果的总隐私成本。形式上，差分隐私的顺序组合定义如下：

- 如果$F_1(x)$ 满足$(\epsilon_1,0)-dp$
- 并且$F_2(x)$ 满足$(\epsilon_2,0)-dp$
-  那么对于机制$G(x)=(F_1(x),F_2(x))$，它满足$(\epsilon_1+\epsilon_2,0)-dp$

    顺序组合是差分隐私的一个重要属性，因为它是能够多次查询数据的基础。当对单一数据集进行多个独立分析时，顺序组合也很重要，因为它允许个人约束他们参与所有这些分析所产生的总隐私成本。顺序组合给出的隐私成本约束是一个上限，多个特定的不同的隐私方法的实际隐私成本可能小于此，但绝不会大于此。

## 平行组合(Parallel Composition)

​    差分隐私的第二个重要属性被称为平行组合。并行组合可以被看作是顺序组合的替代品，它是计算多个数据发布的总隐私成本的第二种方式。并行组合是基于将你的数据集分割成不相干的块，并在每个块上分别运行不同的隐私机制的想法。由于这些块是不相连的，每个人的数据正好出现在一个块中，因此，即使总共有k个块（因此有k个机制的运行），该机制对每个人的数据正好运行一次。从形式上看。

- 如果$F(x)$ 满足$(\epsilon,0)-dp$

- 如果我们将一个数据集$X$分为$k$个不相关的子块，即$x_1\cup...\cup x_k=X$

- 那么对于所有分开的机制$F(x_1),...,F(x_k)$满足$(\epsilon,0)-dp$

    从效果上看，这是一个比顺序组合要好得多的约束。由于我们运行$F k$次，顺序组合会说这个程序满足$(\epsilon,0)-dp$差分隐私。也就是说，总的隐私成本只有ϵ。
    
    形式上的定义与我们的直觉相吻合，如果数据集中的每个参与者都为$X$贡献了一行，那么这一行将正好出现在$x_1,...,x_k$的一个块中。这意味着$F$只会 "看到 "这个参与者的数据一次，这意味着$ϵ$的隐私成本对该个体是合适的。由于这一属性对所有个人都适用,所以每部分的隐私成本都是$ϵ$。

## 后处理(Post-processing)

   我们在这里要讨论的差异化隐私的第三个属性称为后处理。这个想法很简单：通过以某种方式对数据进行后处理，对于处理后的数据来讲，差分隐私对于数据的隐私保护效果依然存在，即

- 如果$F(x)$ 满足$(\epsilon,0)-dp$
- 那么对于任何（确定的或随机的）函数$g$，$g(F(X))$满足$(\epsilon,0)-dp$

    后处理属性意味着无论对不同隐私机制的输出进行什么计算，它都是安全的，不存在因此带来的隐私保护的危险，特别是，在机制的输出中进行可能减少噪音或改善信号的后处理是没有问题的（例如，对于不应该返回负面结果的查询，用零替换负面结果）。事实上，许多复杂的差分隐私算法都利用后处理来减少噪音并提高其结果的准确性。        
    
    后处理属性的另一个含义是，差分隐私提供了对基于辅助信息的隐私攻击的抵抗。例如，函数g可能包含关于数据集元素的辅助信息，并试图利用这些信息进行链接攻击，此时后处理属性就能够保证，这种攻击的有效性受到隐私参数ϵ的限制，而不考虑g中包含的辅助信息。

## 组合定理（Composition Theorem）

之前的文章说的都是对于单个函数而言，而在实际情况中，往往不是只有单个的查询的，所以就有了组合定理，即Composition Theorem，它探讨的就是在多个函数时，要满足什么样的隐私呢？

如果我们已知一个随机函数的隐私损失，则k 个随机函数总的隐私损失是多少呢？通过上面的叙述，我们地发现，在每一个随机函数所加的噪声（noise）相互独立时，隐私损失是可以累加的，即简单组合定理（Simple Composition）。

### 简单组合定理

简单组合定理定义如下：

如果$M_i$是$(\epsilon,0)-dp$ 的随机函数，那么一个组合 (M1,M2,...,Mk) 满足$(\sum_{i=1}^k\epsilon,0)-dp$.

如果$M_i$是$(\epsilon_i,\delta_i)-dp$ 的随机函数，那么一个组合 (M1,M2,...,Mk) 满足$(\sum_{i=1}^k\epsilon_i,\sum_{i=1}^k\delta_i)-dp$.

 上述方法可以得到一个简单的上界，但是显然这是不够的，因为隐私损失是一个随机变量，要获得多个随机变量累加的上界，绝对不是简单的把每一个随机变量的上界累加就可以得到的。注意到**随机变量除了本身的值有一个范围，他的期望也有一个范围**。**如果同时考虑这两个范围，那么就很可能得到一个更紧的上界**。

### 进阶组合定理（Advanced Composition）

它的定义如下

对于$\forall\epsilon,\delta,\delta^\prime\geq0$,如果$M_i$是$(\epsilon,\delta)-dp$ 的随机函数，那么$(M_1,M_2...,M_k)$满足$(\epsilon^\prime,k\delta+\delta^\prime)-dp$ ，其中$\epsilon^\prime=\sqrt{2kln({1\over\epsilon^\prime})}\epsilon+k\epsilon(e^\epsilon-1)$

可以看到，总的$\epsilon$不再像Simple Composition一样随着随机函数的数量$k$线性增长，而是随着$\sqrt{k}$线性增长，因此Advanced Composition在$k$较大时显著好于Simple Composition。

## Moments Accountant

但是对于差分隐私组合来讲，要获得多个随机变量累加的上界，绝对不是简单的把每一个随机变量的上界累加就可以得到的。随机变量除了本身的值有一个范围，他的期望也有一个范围。如果同时考虑这两个范围，那么就很可能得到一个更紧的上界。

于是Abadi等人提出了具有更紧上界的Moments Accountant，为了证明这个上界，首先需要证明每一个随机函数的矩生成函数的上界。Abadi et al.针对高斯分布的特例，使用了二项展开的方法，证明了前三项远大于其他的项和前三项的上界。之后他们又证明了矩生成函数的上界可以像隐私损失一样累加（在噪声独立的情况下）。最终，他们导出了一个更紧的composition隐私损失的上界。由于他们的研究着重于深度学习，因此给出的定理是以SGD为基础的。

Moments Accountant定义如下：

假设SGD的随机选择概率为 q ，训练轮数为 T ，那么存在常数 c1,c2 使得 ∀ε<c1  ,∀δ>0 ，若Gaussian Mechanism的标准差 σ 满足

$\sigma\geq c_2q \sqrt {Tlog({1\over\delta})}$

则这一系列 T 个 Gaussian Mechanism Mi 所构成的composition满足 $(\epsilon_i,\delta_i)-dp$ 

这个方法也是目前最先进的差分隐私核算方法。

参考文献

[Properties of Differential Privacy — Programming Differential Privacy (programming-dp.com)](https://programming-dp.com/notebooks/ch4.html#post-processing)

[差分隐私之Composition Theorem（一） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/264779199)
