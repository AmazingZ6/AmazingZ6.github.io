---
title: 差分隐私定义
date: 2022-09-16 19:31:08
tags: 差分隐私
plugins:
  - mathjax
---

# 差分隐私学习001



## 1 普通差分隐私的定义

差分隐私中使用KL散度（KL-Divergence）来衡量隐私

所谓KL散度，开始是用于度量我们拟合的损失与与实际观察到的损失的差距，也可以用于表示当某分布q(x)被用于近似p(x)时的信息损失，它表示如下。
$$
D(p||q)=\sum_{x \in y}p(x)log{p(x)\over q(x)}
$$
也就是说，q(x)能在多大程度上表达p(x)所包含的信息，KL散度越大，表示q(x)能表示的p(x)越少，则表达效果越差。

但是我们需要的不是这两个分布的整体差异，我们只需要这两个分布在差距最大的情况下能够满足就行。





所以有了MAX-Divergence, 即
$$
D_\infty(p||q)=\underset {S\subseteq Supp(Y)}{max} [ln{Pr(Y \in S) \over Pr(Z \in S)}]
$$

上式经过变形整理可得
$$
Pr[M(x)\in S] \leq e^\varepsilon Pr[M(x^\prime)\in S]
$$


但是上述差分隐私定义太过严格，所以为了算法的实用性，引入了松弛版本的差分隐私（δ-Approximate Max Divergence），定义如下
$$
D^{\delta}_\infty(p||q)=\underset {S\subseteq Supp(Y):Pr[Y \in S]\geq \delta}{max} [ln{Pr(Y \in S) - \delta \over Pr(Z \in S)}]
$$
新的隐私参数δ，代表了定义的失败概率，即有1-δ的概率，我们将得到与普通差分隐私相同的保证；有δ的概率，我们将得不到隐私保证。换言之，上述式子将分子减去了一个δ，相当于有了一定的放缩空间。

也就是说，有δ的概率，任何事情都可能发生，甚至是整个数据集的泄露，因此，我们通常要求δ非常小,通常为
$$
1\over n^2
$$
或更小，其中n为数据集的大小。

因此，我们将看到，实际使用中的(ϵ,δ)差分隐私不会像刚刚说的那样的整个数据集的泄露，相反，一般来说，失败所付出的代价是极小的。

差分隐私所要求的就是需要上述散度小于或等于隐私界限，即
$$
D^{\delta}_\infty(p||q) \leq \varepsilon
$$
 ϵ参数被称为隐私参数或隐私预算，用户可以通过改变它来调整该定义提供的 "隐私量"。ϵ越小，就要求对于相邻输入提供越相似的输出，以此来达到更好的隐私水平，ϵ很大时，就使得相邻输入的输出的相似性要求很小，因此提供的隐私水平较低。

同样，上式整理可得
$$
Pr[M(x)\in S] \leq e^\varepsilon Pr[M(x^\prime)\in S]+\delta
$$

## 2 贝叶斯差分隐私定义

参照普通差分隐私，贝叶斯差分隐私普通定义如下                               
$$
Pr[M(x)\in S] \leq e^{\varepsilon_\mu} Pr[M(x^\prime)\in S]+\delta_\mu
$$
还有一种定义是
$$
Pr[L(w,X,X^\prime)\geq \varepsilon_\mu] \leq \delta_\mu
$$
其中

$$
F(x)=f(x)+Lap({s\over\epsilon})
$$





参考资料

[差分隐私（一） Differential Privacy 简介 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/139114240)

[^](https://zhuanlan.zhihu.com/p/139114240#ref_1_0)The Algorithmic Foundations of Differential Privacy https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf
